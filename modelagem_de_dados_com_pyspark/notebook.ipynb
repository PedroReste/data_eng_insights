{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2723e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "spark = SparkSession.builder.appName(\"session\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\") #Otimização dinâmicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e4dae",
   "metadata": {},
   "source": [
    "# Conversão de Arquivos CSV para Parquet\n",
    "No uso do PySpark, ainda mais em ambientes de cloud, para diminuir o espaço utilziado e processamento futuro, o ideal usar o formato **Parquet** ao invés de **CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212d5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando os arquivos em CSV com a função nativa do PySpark\n",
    "olist_customers = spark.read.option(\"header\", True).csv(\"olist_customers_dataset.csv\")\n",
    "olist_orders = spark.read.option(\"header\", True).csv(\"olist_orders_dataset.csv\")\n",
    "olist_order_reviews = spark.read.option(\"header\", True).csv(\"olist_order_reviews_dataset.csv\")\n",
    "olist_order_items = spark.read.option(\"header\", True).csv(\"olist_order_items_dataset.csv\")\n",
    "olist_sellers = spark.read.option(\"header\", True).csv(\"olist_sellers_dataset.csv\")\n",
    "olist_products = spark.read.option(\"header\", True).csv(\"olist_products_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95d50f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertendo os dfs para Parquet\n",
    "olist_customers.write.mode(\"overwrite\").parquet(\"olist_customers_dataset.parquet\")\n",
    "olist_customers.write.mode(\"overwrite\").parquet(\"olist_customers_dataset.parquet\")\n",
    "olist_orders.write.mode(\"overwrite\").parquet(\"olist_orders_dataset.parquet\")\n",
    "olist_order_reviews.write.mode(\"overwrite\").parquet(\"olist_order_reviews_dataset.parquet\")\n",
    "olist_order_items.write.mode(\"overwrite\").parquet(\"olist_order_items_dataset.parquet\")\n",
    "olist_sellers.write.mode(\"overwrite\").parquet(\"olist_sellers_dataset.parquet\")\n",
    "olist_products.write.mode(\"overwrite\").parquet(\"olist_products_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6851ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando novamente os dfs, mas com a versão em Parquet\n",
    "olist_customers = spark.read.option(\"header\", True).parquet(\"olist_customers_dataset.parquet\")\n",
    "olist_orders = spark.read.option(\"header\", True).parquet(\"olist_orders_dataset.parquet\")\n",
    "olist_order_reviews = spark.read.option(\"header\", True).parquet(\"olist_order_reviews_dataset.parquet\")\n",
    "olist_order_items = spark.read.option(\"header\", True).parquet(\"olist_order_items_dataset.parquet\")\n",
    "olist_sellers = spark.read.option(\"header\", True).parquet(\"olist_sellers_dataset.parquet\")\n",
    "olist_products = spark.read.option(\"header\", True).parquet(\"olist_products_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebb8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo: olist_customers\n",
      "  CSV:     8.71 MB\n",
      "  Parquet: 0.00 MB\n",
      "\n",
      "Arquivo: olist_orders\n",
      "  CSV:     16.93 MB\n",
      "  Parquet: 0.00 MB\n",
      "\n",
      "Arquivo: olist_order_reviews\n",
      "  CSV:     13.78 MB\n",
      "  Parquet: 0.00 MB\n",
      "\n",
      "Arquivo: olist_order_items\n",
      "  CSV:     14.83 MB\n",
      "  Parquet: 0.00 MB\n",
      "\n",
      "Arquivo: olist_sellers\n",
      "  CSV:     0.17 MB\n",
      "  Parquet: 0.00 MB\n",
      "\n",
      "Arquivo: olist_products\n",
      "  CSV:     2.30 MB\n",
      "  Parquet: 0.00 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datasets = {\n",
    "    \"olist_customers\": olist_customers,\n",
    "    \"olist_orders\": olist_orders,\n",
    "    \"olist_order_reviews\": olist_order_reviews,\n",
    "    \"olist_order_items\": olist_order_items,\n",
    "    \"olist_sellers\": olist_sellers,\n",
    "    \"olist_products\": olist_products\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    csv_path = f\"{name}_dataset.csv\"\n",
    "    parquet_path = f\"{name}_dataset.parquet\"\n",
    "    \n",
    "    csv_size = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "    parquet_size = os.path.getsize(parquet_path) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Arquivo: {name}\")\n",
    "    print(f\"  CSV:     {csv_size:.2f} MB\")\n",
    "    print(f\"  Parquet: {parquet_size:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e590354",
   "metadata": {},
   "source": [
    "Além do tempo de execução para leitura dos arquivos, que passou de 6,1 segundos para 0,6 segundos na minha máquina. \\\n",
    "Todos os aruivos passaram a pesar menos de 1MB, mostrando o ganho que teria para arquivos e bases em GBs ou até TBs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4b279",
   "metadata": {},
   "source": [
    "# Testes com a base da Olist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d1f914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- order_delivered_carrier_date: string (nullable = true)\n",
      " |-- order_delivered_customer_date: string (nullable = true)\n",
      " |-- order_estimated_delivery_date: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- review_score: string (nullable = true)\n",
      " |-- review_comment_title: string (nullable = true)\n",
      " |-- review_comment_message: string (nullable = true)\n",
      " |-- review_creation_date: string (nullable = true)\n",
      " |-- review_answer_timestamp: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- freight_value: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_category_name: string (nullable = true)\n",
      " |-- product_name_lenght: string (nullable = true)\n",
      " |-- product_description_lenght: string (nullable = true)\n",
      " |-- product_photos_qty: string (nullable = true)\n",
      " |-- product_weight_g: string (nullable = true)\n",
      " |-- product_length_cm: string (nullable = true)\n",
      " |-- product_height_cm: string (nullable = true)\n",
      " |-- product_width_cm: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [olist_customers, olist_orders, olist_order_reviews, \n",
    "            olist_order_items, olist_sellers, olist_products]\n",
    "for df in datasets:\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405e43cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|            order_id|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n",
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|2d846c03073b1a424...|5a3473648da10d280...|   delivered|     2017-05-10 15:18:16|2017-05-12 02:45:22|         2017-05-15 09:14:35|          2017-05-25 10:49:48|          2017-05-18 00:00:00|\n",
      "|398c8e07a36c8e104...|dd9fb9905e0a0a4f0...|   delivered|     2017-07-30 19:07:55|2017-07-30 19:23:30|         2017-08-11 19:02:40|          2017-08-22 18:39:52|          2017-08-31 00:00:00|\n",
      "|33e2db141538ccdaf...|37ff611a66e811a67...|   delivered|     2017-09-05 08:48:29|2017-09-06 02:50:44|         2017-09-19 20:37:56|          2017-09-23 14:33:26|          2017-09-29 00:00:00|\n",
      "|3621b14695412ecce...|fb0d7f836d199425d...|   delivered|     2018-01-29 18:51:19|2018-01-29 19:13:57|         2018-02-01 20:22:10|          2018-02-06 18:22:35|          2018-02-20 00:00:00|\n",
      "|1a949e19da7e100bd...|89cad968aa5731c56...|   delivered|     2017-11-21 16:34:39|2017-11-21 16:51:18|         2017-11-23 16:18:59|          2017-12-04 21:26:52|          2017-12-08 00:00:00|\n",
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olist_orders.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f68d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------+\n",
      "|     status|  qtd|hora_compra|\n",
      "+-----------+-----+-----------+\n",
      "|  delivered|96478|       4060|\n",
      "|    shipped| 1107|         32|\n",
      "|   canceled|  625|         23|\n",
      "|unavailable|  609|         50|\n",
      "|   invoiced|  314|         13|\n",
      "| processing|  301|         16|\n",
      "|    created|    5|          0|\n",
      "|   approved|    2|          0|\n",
      "+-----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olist_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_status AS status,\n",
    "        COUNT(*) AS qtd,\n",
    "        COUNT(CASE WHEN order_purchase_timestamp \n",
    "                  BETWEEN '2017-10-02' AND '2017-10-30'\n",
    "                  THEN 1 ELSE NULL END) AS hora_compra\n",
    "    FROM orders\n",
    "    GROUP BY order_status\n",
    "    SORT BY qtd DESC\n",
    "\"\"\")\n",
    "\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36fd9e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           categoria|produtos|\n",
      "+--------------------+--------+\n",
      "|     cama_mesa_banho|    3029|\n",
      "|       esporte_lazer|    2867|\n",
      "|    moveis_decoracao|    2657|\n",
      "|        beleza_saude|    2444|\n",
      "|utilidades_domest...|    2335|\n",
      "|          automotivo|    1900|\n",
      "|informatica_acess...|    1639|\n",
      "|          brinquedos|    1411|\n",
      "|  relogios_presentes|    1329|\n",
      "|           telefonia|    1134|\n",
      "|               bebes|     919|\n",
      "|          perfumaria|     868|\n",
      "|fashion_bolsas_e_...|     849|\n",
      "|           papelaria|     849|\n",
      "|          cool_stuff|     789|\n",
      "|  ferramentas_jardim|     753|\n",
      "|            pet_shop|     719|\n",
      "|                NULL|     610|\n",
      "|         eletronicos|     517|\n",
      "|construcao_ferram...|     400|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olist_products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_category_name AS categoria,\n",
    "        COUNT(DISTINCT product_id) AS produtos\n",
    "    FROM products\n",
    "    GROUP BY product_category_name\n",
    "    SORT BY produtos DESC\n",
    "\"\"\")\n",
    "\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d798bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|produtos|categoria|\n",
      "+--------+---------+\n",
      "|   32951|       73|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "olist_products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "query = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT product_id) AS produtos,\n",
    "           COUNT(DISTINCT product_category_name) AS categoria\n",
    "    FROM products\n",
    "\"\"\")\n",
    "\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44be01",
   "metadata": {},
   "source": [
    "# 3 maiores salários - Uso do Window\n",
    "\n",
    "A função Window no PySpark é usada para definir janelas de particionamento e ordenação dentro de um DataFrame, possibilitando a aplicação de funções analíticas (window functions), como row_number, rank, lead, lag, sum, avg, etc., sem agrupar os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e935667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Frank|        RH|  6200|   1|\n",
      "|        Grace|        RH|  6100|   2|\n",
      "|          Eve|        RH|  6000|   3|\n",
      "|          Bob|        TI|  7000|   1|\n",
      "|        Carol|        TI|  6500|   2|\n",
      "|        Alice|        TI|  5000|   3|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Top3Salaries\").getOrCreate()\n",
    "\n",
    "# Exemplo de DataFrame\n",
    "data = [\n",
    "    (\"Alice\", \"TI\", 5000),\n",
    "    (\"Bob\", \"TI\", 7000),\n",
    "    (\"Carol\", \"TI\", 6500),\n",
    "    (\"Dave\", \"RH\", 4500),\n",
    "    (\"Eve\", \"RH\", 6000),\n",
    "    (\"Frank\", \"RH\", 6200),\n",
    "    (\"Grace\", \"RH\", 6100)\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"employee_name\", \"department\", \"salary\"])\n",
    "\n",
    "# Janela e ranking\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "df.filter(col(\"rank\") <= 3).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a44e84",
   "metadata": {},
   "source": [
    "# Linhas para Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901ed73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+\n",
      "|employee| Feb| Jan|\n",
      "+--------+----+----+\n",
      "|     Bob|2900|2800|\n",
      "|   Alice|3200|3000|\n",
      "+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Criar SparkSession\n",
    "spark = SparkSession.builder.appName(\"RowToColumn\").getOrCreate()\n",
    "\n",
    "# Dados de exemplo\n",
    "data = [\n",
    "    (\"Alice\", \"Jan\", 3000),\n",
    "    (\"Alice\", \"Feb\", 3200),\n",
    "    (\"Bob\", \"Jan\", 2800),\n",
    "    (\"Bob\", \"Feb\", 2900),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"employee\", \"month\", \"salary\"])\n",
    "\n",
    "# Pivot\n",
    "pivot_df = df.groupBy(\"employee\").pivot(\"month\").sum(\"salary\")\n",
    "\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad1ebbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+\n",
      "|employee| Jan| Feb|\n",
      "+--------+----+----+\n",
      "|   Alice|3000|3200|\n",
      "|     Bob|2800|2900|\n",
      "+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"salaries\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "      employee,\n",
    "      SUM(CASE WHEN month = 'Jan' THEN salary END) AS Jan,\n",
    "      SUM(CASE WHEN month = 'Feb' THEN salary END) AS Feb\n",
    "    FROM salaries\n",
    "    GROUP BY employee\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8ce2d",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841de247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"categoria\", outputCol=\"categoria_indexada\")\n",
    "df_indexado = indexer.fit(df).transform(df)\n",
    "\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"categoria_indexada\"], outputCols=[\"categoria_vetorizada\"])\n",
    "df_encoded = encoder.fit(df_indexado).transform(df_indexado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58cef35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7df45d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+-------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|stringIndexer|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+-------------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|              franca|            SP|          0.0|\n",
      "|18955e83d337fd6b2...|290c77bc529b7ac93...|                   09790|sao bernardo do c...|            SP|          0.0|\n",
      "|4e7b3e00288586ebd...|060e732b5b29e8181...|                   01151|           sao paulo|            SP|          0.0|\n",
      "|b2b6027bc5c5109e5...|259dac757896d24d7...|                   08775|     mogi das cruzes|            SP|          0.0|\n",
      "|4f2d8ab171c80ec83...|345ecd01c38d18a90...|                   13056|            campinas|            SP|          0.0|\n",
      "|879864dab9bc30475...|4c93744516667ad3b...|                   89254|      jaragua do sul|            SC|          5.0|\n",
      "|fd826e7cf63160e53...|addec96d2e059c80c...|                   04534|           sao paulo|            SP|          0.0|\n",
      "|5e274e7a0c3809e14...|57b2a98a409812fe9...|                   35182|             timoteo|            MG|          2.0|\n",
      "|5adf08e34b2e99398...|1175e95fb47ddff9d...|                   81560|            curitiba|            PR|          4.0|\n",
      "|4b7139f34592b3a31...|9afe194fb833f79e3...|                   30575|      belo horizonte|            MG|          2.0|\n",
      "|9fb35e4ed6f0a14a4...|2a7745e1ed516b289...|                   39400|       montes claros|            MG|          2.0|\n",
      "|5aa9e4fdd4dfd2095...|2a46fb94aef5cbeeb...|                   20231|      rio de janeiro|            RJ|          1.0|\n",
      "|b2d1536598b73a9ab...|918dc87cd72cd9f6e...|                   18682|    lencois paulista|            SP|          0.0|\n",
      "|eabebad39a88bb6f5...|295c05e81917928d7...|                   05704|           sao paulo|            SP|          0.0|\n",
      "|1f1c7bf1c9b041b29...|3151a81801c838636...|                   95110|       caxias do sul|            RS|          3.0|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+-------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"customer_state\", outputCol=\"stringIndexer\")\n",
    "df_indexado = indexer.fit(olist_customers).transform(olist_customers)\n",
    "df_indexado.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b497ea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+-------------+---------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|stringIndexer| oneHotEnconder|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+-------------+---------------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|              franca|            SP|          0.0| (26,[0],[1.0])|\n",
      "|18955e83d337fd6b2...|290c77bc529b7ac93...|                   09790|sao bernardo do c...|            SP|          0.0| (26,[0],[1.0])|\n",
      "|4e7b3e00288586ebd...|060e732b5b29e8181...|                   01151|           sao paulo|            SP|          0.0| (26,[0],[1.0])|\n",
      "|b2b6027bc5c5109e5...|259dac757896d24d7...|                   08775|     mogi das cruzes|            SP|          0.0| (26,[0],[1.0])|\n",
      "|4f2d8ab171c80ec83...|345ecd01c38d18a90...|                   13056|            campinas|            SP|          0.0| (26,[0],[1.0])|\n",
      "|879864dab9bc30475...|4c93744516667ad3b...|                   89254|      jaragua do sul|            SC|          5.0| (26,[5],[1.0])|\n",
      "|fd826e7cf63160e53...|addec96d2e059c80c...|                   04534|           sao paulo|            SP|          0.0| (26,[0],[1.0])|\n",
      "|5e274e7a0c3809e14...|57b2a98a409812fe9...|                   35182|             timoteo|            MG|          2.0| (26,[2],[1.0])|\n",
      "|5adf08e34b2e99398...|1175e95fb47ddff9d...|                   81560|            curitiba|            PR|          4.0| (26,[4],[1.0])|\n",
      "|4b7139f34592b3a31...|9afe194fb833f79e3...|                   30575|      belo horizonte|            MG|          2.0| (26,[2],[1.0])|\n",
      "|9fb35e4ed6f0a14a4...|2a7745e1ed516b289...|                   39400|       montes claros|            MG|          2.0| (26,[2],[1.0])|\n",
      "|5aa9e4fdd4dfd2095...|2a46fb94aef5cbeeb...|                   20231|      rio de janeiro|            RJ|          1.0| (26,[1],[1.0])|\n",
      "|b2d1536598b73a9ab...|918dc87cd72cd9f6e...|                   18682|    lencois paulista|            SP|          0.0| (26,[0],[1.0])|\n",
      "|eabebad39a88bb6f5...|295c05e81917928d7...|                   05704|           sao paulo|            SP|          0.0| (26,[0],[1.0])|\n",
      "|1f1c7bf1c9b041b29...|3151a81801c838636...|                   95110|       caxias do sul|            RS|          3.0| (26,[3],[1.0])|\n",
      "|206f3129c0e4d7d0b...|21f748a16f4e1688a...|                   13412|          piracicaba|            SP|          0.0| (26,[0],[1.0])|\n",
      "|a7c125a0a07b75146...|5c2991dbd08bbf3cf...|                   22750|      rio de janeiro|            RJ|          1.0| (26,[1],[1.0])|\n",
      "|c5c61596a3b6bd0ce...|b6e99561fe6f34a55...|                   07124|           guarulhos|            SP|          0.0| (26,[0],[1.0])|\n",
      "|9b8ce803689b3562d...|7f3a72e8f988c6e73...|                   05416|           sao paulo|            SP|          0.0| (26,[0],[1.0])|\n",
      "|49d0ea0986edde72d...|3e6fd6b2f0d499456...|                   68485|              pacaja|            PA|         12.0|(26,[12],[1.0])|\n",
      "|154c4ded6991bdfa3...|e607ede0e63436308...|                   88034|       florianopolis|            SC|          5.0| (26,[5],[1.0])|\n",
      "|690172ab319622688...|a96d5cfa0d3181817...|                   74914|aparecida de goiania|            GO|          9.0| (26,[9],[1.0])|\n",
      "|2938121a40a20953c...|482441ea6a06b1f72...|                   05713|           sao paulo|            SP|          0.0| (26,[0],[1.0])|\n",
      "|237098a64674ae89b...|4390ddbb6276a66ff...|                   82820|            curitiba|            PR|          4.0| (26,[4],[1.0])|\n",
      "|cb721d7b4f271fd87...|a5844ba4bfc8d0cc6...|                   08225|           sao paulo|            SP|          0.0| (26,[0],[1.0])|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+-------------+---------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCols=[\"stringIndexer\"], outputCols=[\"oneHotEnconder\"])\n",
    "df_encoded = encoder.fit(df_indexado).transform(df_indexado)\n",
    "df_encoded.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86010a3",
   "metadata": {},
   "source": [
    "# Exemplo de Pipeline e Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a340954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PipelineExemplo\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"A\", 25.0, 50000.0, 1.0),\n",
    "    (\"B\", 45.0, 60000.0, 0.0),\n",
    "    (\"A\", 35.0, 65000.0, 1.0),\n",
    "    (\"C\", 23.0, 52000.0, 0.0),\n",
    "    (\"B\", 52.0, 70000.0, 1.0)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"idade\", DoubleType(), True),\n",
    "    StructField(\"renda\", DoubleType(), True),\n",
    "    StructField(\"label\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20ded33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# 1. StringIndexer para variável categórica\n",
    "indexer = StringIndexer(inputCol=\"categoria\", outputCol=\"categoria_index\")\n",
    "\n",
    "# 2. OneHotEncoder para a coluna indexada\n",
    "encoder = OneHotEncoder(inputCols=[\"categoria_index\"], outputCols=[\"categoria_ohe\"])\n",
    "\n",
    "# 3. VectorAssembler para juntar todas as features em um vetor\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"categoria_ohe\", \"idade\", \"renda\"], \n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "# 4. MinMaxScaler para normalizar as features\n",
    "scaler = MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "# 5. Modelo Estimador\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# 6. Pipeline com todas as etapas\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, scaler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28e5a999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-------+--------------------+----------+--------------------+\n",
      "|categoria|idade|  renda|            features|prediction|         probability|\n",
      "+---------+-----+-------+--------------------+----------+--------------------+\n",
      "|        A| 25.0|50000.0|[1.0,0.0,0.068965...|       1.0|[1.97174813557347...|\n",
      "|        B| 45.0|60000.0|[0.0,1.0,0.758620...|       0.0|[0.99999999038469...|\n",
      "|        A| 35.0|65000.0|[1.0,0.0,0.413793...|       1.0|[2.34638336882296...|\n",
      "|        C| 23.0|52000.0|       (4,[3],[0.1])|       0.0|           [1.0,0.0]|\n",
      "|        B| 52.0|70000.0|   [0.0,1.0,1.0,1.0]|       1.0|[9.19521084368343...|\n",
      "+---------+-----+-------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(df)  # PipelineModel (estimator aplicado)\n",
    "\n",
    "# Transformar os dados de entrada e fazer previsões\n",
    "result = pipeline_model.transform(df)\n",
    "\n",
    "# Mostrar resultados\n",
    "result.select(\"categoria\", \"idade\", \"renda\", \"features\", \"prediction\", \"probability\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04139597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|categoria|categoria_index|\n",
      "+---------+---------------+\n",
      "|        A|            0.0|\n",
      "|        B|            1.0|\n",
      "|        A|            0.0|\n",
      "|        C|            2.0|\n",
      "|        B|            1.0|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"categoria\", outputCol=\"categoria_index\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "df_indexed.select(\"categoria\", \"categoria_index\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4722df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|categoria_index|categoria_ohe|\n",
      "+---------------+-------------+\n",
      "|0.0            |(2,[0],[1.0])|\n",
      "|1.0            |(2,[1],[1.0])|\n",
      "|0.0            |(2,[0],[1.0])|\n",
      "|2.0            |(2,[],[])    |\n",
      "|1.0            |(2,[1],[1.0])|\n",
      "+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCols=[\"categoria_index\"], outputCols=[\"categoria_ohe\"])\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded.select(\"categoria_index\", \"categoria_ohe\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be5ef3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-------+-------------+----------------------+\n",
      "|categoria|idade|renda  |categoria_ohe|features_raw          |\n",
      "+---------+-----+-------+-------------+----------------------+\n",
      "|A        |25.0 |50000.0|(2,[0],[1.0])|[1.0,0.0,25.0,50000.0]|\n",
      "|B        |45.0 |60000.0|(2,[1],[1.0])|[0.0,1.0,45.0,60000.0]|\n",
      "|A        |35.0 |65000.0|(2,[0],[1.0])|[1.0,0.0,35.0,65000.0]|\n",
      "|C        |23.0 |52000.0|(2,[],[])    |[0.0,0.0,23.0,52000.0]|\n",
      "|B        |52.0 |70000.0|(2,[1],[1.0])|[0.0,1.0,52.0,70000.0]|\n",
      "+---------+-----+-------+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"categoria_ohe\", \"idade\", \"renda\"], outputCol=\"features_raw\")\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "df_assembled.select(\"categoria\", \"idade\", \"renda\", \"categoria_ohe\", \"features_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad290a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+----------------------------------+\n",
      "|features_raw          |features                          |\n",
      "+----------------------+----------------------------------+\n",
      "|[1.0,0.0,25.0,50000.0]|[1.0,0.0,0.06896551724137931,0.0] |\n",
      "|[0.0,1.0,45.0,60000.0]|[0.0,1.0,0.7586206896551724,0.5]  |\n",
      "|[1.0,0.0,35.0,65000.0]|[1.0,0.0,0.41379310344827586,0.75]|\n",
      "|[0.0,0.0,23.0,52000.0]|(4,[3],[0.1])                     |\n",
      "|[0.0,1.0,52.0,70000.0]|[0.0,1.0,1.0,1.0]                 |\n",
      "+----------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "df_scaled.select(\"features_raw\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4fe7f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Computador\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Computador\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[indexer, encoder, assembler, scaler, lr])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Treinando e gerando o modelo final\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pipeline_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Salvando o PipelineModel treinado\u001b[39;00m\n\u001b[0;32m     10\u001b[0m pipeline_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeu_pipeline_modelo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Definindo o pipeline com todas as etapas\n",
    "pipeline = Pipeline(stages=[indexer, encoder, assembler, scaler, lr])\n",
    "\n",
    "# Treinando e gerando o modelo final\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "# Salvando o PipelineModel treinado\n",
    "pipeline_model.save(\"meu_pipeline_modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb582b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Carregando o modelo salvo\n",
    "modelo_carregado = PipelineModel.load(\"meu_pipeline_modelo\")\n",
    "\n",
    "# Aplicando em novos dados (deve ter mesmas colunas e tipos)\n",
    "novos_dados = spark.createDataFrame([\n",
    "    (\"B\", 40.0, 58000.0, 1.0),\n",
    "    (\"C\", 22.0, 49000.0, 0.0)\n",
    "], [\"categoria\", \"idade\", \"renda\", \"label\"])\n",
    "\n",
    "# Transformando os novos dados\n",
    "resultado = modelo_carregado.transform(novos_dados)\n",
    "resultado.select(\"features\", \"prediction\", \"probability\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f26ef",
   "metadata": {},
   "source": [
    "# Exemplos de Otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca4b8233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PipelineExemplo\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"A\", 25.0, 50000.0, 1.0),\n",
    "    (\"B\", 45.0, 60000.0, 0.0),\n",
    "    (\"A\", 35.0, 65000.0, 1.0),\n",
    "    (\"C\", 23.0, 52000.0, 0.0),\n",
    "    (\"B\", 52.0, 70000.0, 1.0)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"categoria\", StringType(), True),\n",
    "    StructField(\"idade\", DoubleType(), True),\n",
    "    StructField(\"renda\", DoubleType(), True),\n",
    "    StructField(\"label\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f8ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"dados.parquet\")\n",
    "df.cache()  # ou df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df.count()  # primeira ação dispara o cache\n",
    "df.select(\"coluna\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f787e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-------+-----+\n",
      "|categoria|idade|  renda|label|\n",
      "+---------+-----+-------+-----+\n",
      "|        C| 23.0|52000.0|  0.0|\n",
      "|        A| 35.0|65000.0|  1.0|\n",
      "|        B| 52.0|70000.0|  1.0|\n",
      "|        A| 25.0|50000.0|  1.0|\n",
      "|        B| 45.0|60000.0|  0.0|\n",
      "+---------+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_repart = df.repartition(10, \"idade\")\n",
    "df_repart.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae2ef43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[categoria: string, idade: double, renda: double, label: double]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coalesced = df.coalesce(2)\n",
    "df_coalesced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_grande = spark.read.parquet(\"grande.parquet\")\n",
    "df_pequeno = spark.read.parquet(\"referencia.parquet\")\n",
    "\n",
    "df_join = df_grande.join(broadcast(df_pequeno), \"chave\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec69aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 100)  # padrão é 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76686a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"grande.parquet\").select(\"id\", \"nome\").filter(\"id > 1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat, rand\n",
    "\n",
    "# Adiciona \"sal\" à chave\n",
    "df1 = df1.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n",
    "df1 = df1.withColumn(\"chave_salgada\", concat(\"chave\", \"salt\"))\n",
    "\n",
    "# Duplica df2 com sal\n",
    "salts = spark.range(0, 10).withColumnRenamed(\"id\", \"salt\")\n",
    "df2_exp = df2.crossJoin(salts)\n",
    "df2_exp = df2_exp.withColumn(\"chave_salgada\", concat(\"chave\", \"salt\"))\n",
    "\n",
    "# Faz o join usando chave salgada\n",
    "df_join = df1.join(df2_exp, \"chave_salgada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Bom\n",
    "df = df.withColumn(\"novo_valor\", col(\"valor\") * 2)\n",
    "\n",
    "# Evite\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def multiplica(x):\n",
    "    return x * 2\n",
    "\n",
    "multiplica_udf = udf(multiplica, IntegerType())\n",
    "df = df.withColumn(\"novo_valor\", multiplica_udf(col(\"valor\")))  # Menos eficiente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
